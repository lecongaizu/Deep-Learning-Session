{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CNN","provenance":[{"file_id":"1YhcJiTefeN-9cctzUIT4VSlCb1QjgqJc","timestamp":1593781184598},{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/cnn.ipynb","timestamp":1580309224336}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DSPCom-KmApV"},"source":["# Convolutional Neural Network (CNN)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qLGkt5qiyz4E"},"source":["Convolutional neural networks. Sounds like a weird combination of biology and math with a little CS sprinkled in, but these networks have been some of the most influential innovations in the field of computer vision. 2012 was the first year that neural nets grew to prominence as Alex Krizhevsky used them to win that year’s ImageNet competition (basically, the annual Olympics of computer vision), dropping the classification error record from 26% to 15%, an astounding improvement at the time.Ever since then, a host of companies have been using deep learning at the core of their services. Facebook uses neural nets for their automatic tagging algorithms, Google for their photo search, Amazon for their product recommendations, Pinterest for their home feed personalization, and Instagram for their search infrastructure."]},{"cell_type":"markdown","metadata":{"id":"IXFD6kD7yAPR","colab_type":"text"},"source":["**The Problem Space**\n","\n","Image classification is the task of taking an input image and outputting a class (a cat, dog, etc) or a probability of classes that best describes the image. For humans, this task of recognition is one of the first skills we learn from the moment we are born and is one that comes naturally and effortlessly as adults. Without even thinking twice, we’re able to quickly and seamlessly identify the environment we are in as well as the objects that surround us. When we see an image or just when we look at the world around us, most of the time we are able to immediately characterize the scene and give each object a label, all without even consciously noticing. These skills of being able to quickly recognize patterns, generalize from prior knowledge, and adapt to different image environments are ones that we do not share with our fellow machines."]},{"cell_type":"markdown","metadata":{"id":"tZlDHzzPyXKp","colab_type":"text"},"source":["# **How does it work**\n","\n","A more detailed overview of what CNNs do would be that you take the image, pass it through a series of convolutional, nonlinear, pooling (downsampling), and fully connected layers, and get an output. As we said earlier, the output can be a single class or a probability of classes that best describes the image. Now, the hard part is understanding what each of these layers do. So let’s get into the most important one."]},{"cell_type":"markdown","metadata":{"id":"2T6sqaGuypOA","colab_type":"text"},"source":["** **bold text**The information flow**\n","\n","The first layer in a CNN is always a Convolutional Layer. First thing to make sure you remember is what the input to this conv layer is. lets say the input is an image with size is 32 by 32 and it has 3 color channels therefore the input is a 32 x 32 x 3 array of pixel values. Now, the best way to explain a conv layer is to imagine a flashlight that is shining over the top left of the image. Let’s say that the light this flashlight shines covers a 5 x 5 area. And now, let’s imagine this flashlight sliding across all the areas of the input image. In machine learning terms, this flashlight is called a filter(or sometimes referred to as a neuron or a kernel) and the region that it is shining over is called the receptive field. Now this filter is also an array of numbers (the numbers are called weights or parameters). A very important note is that the depth of this filter has to be the same as the depth of the input (this makes sure that the math works out), so the dimensions of this filter is 5 x 5 x 3. \n","\n","\n","Now, let’s take the first position the filter is in for example.  \n","![alt text](https://adeshpande3.github.io/assets/ActivationMap.png)\n","\n","\n","It would be the top left corner. As the filter is sliding, or convolving, around the input image, it is multiplying the values in the filter with the original pixel values of the image (aka computing element wise multiplications). These multiplications are all summed up. So now you have a single number. Remember, this number is just representative of when the filter is at the top left of the image. Now, we repeat this process for every location on the input volume. (Next step would be moving the filter to the right by 1 unit, then right again by 1, and so on). Every unique location on the input volume produces a number. After sliding the filter over all the locations, you will find out that what you’re left with is a 28 x 28 x 1 array of numbers, which we call an activation map or feature map. The reason you get a 28 x 28 array is that there are 784 different locations that a 5 x 5 filter can fit on a 32 x 32 input image. These 784 numbers are mapped to a 28 x 28 array.\n","\n","Each of these filters can be thought of as feature identifiers. When I say features, I’m talking about things like straight edges, simple colors, and curves. Think about the simplest characteristics that all images have in common with each other. Let’s say our first filter is 7 x 7 x 3 and is going to be a curve detector. (In this section, let’s ignore the fact that the filter is 3 units deep and only consider the top depth slice of the filter and the image, for simplicity.)As a curve detector, the filter will have a pixel structure in which there will be higher numerical values along the area that is a shape of a curve (Remember, these filters that we’re talking about as just numbers!). \n","\n","\n","\n","![alt text](https://adeshpande3.github.io/assets/Filter.png)\n","\n","\n","In reality those values are much lower! This is because there wasn’t anything in the image section that responded to the curve detector filter. Remember, the output of this conv layer is an activation map. So, in the simple case of a one filter convolution (and if that filter is a curve detector), the activation map will show the areas in which there at mostly likely to be curves in the picture. In this example, the top left value of our 26 x 26 x 1 activation map (26 because of the 7x7 filter instead of 5x5) will be 6600. This high value means that it is likely that there is some sort of curve in the input volume that caused the filter to activate. The top right value in our activation map will be 0 because there wasn’t anything in the input volume that caused the filter to activate (or more simply said, there wasn’t a curve in that region of the original image). Remember, this is just for one filter. This is just a filter that is going to detect lines that curve outward and to the right. We can have other filters for lines that curve to the left or for straight edges. The more filters, the greater the depth of the activation map, and the more information we have about the input volume."]},{"cell_type":"markdown","metadata":{"id":"OmFHV1Nn1mz_","colab_type":"text"},"source":["** **bold text**Fully Connected Layer**\n","\n","After the previous edges has been calcuated its time to give those edges a meaning that our netwokr can use to learn. To do so we will add a lasy layer which is called fully connected layer or dense layer (similar to the one we used in our previous classes). This layer basically takes an input volume (whatever the output is of the conv or ReLU or pool layer preceding it) and outputs an N dimensional vector where N is the number of classes that the program has to choose from. For example, if you wanted a digit classification program, N would be 10 since there are 10 digits. Each number in this N dimensional vector represents the probability of a certain class. For example, if the resulting vector for a digit classification program is [0 .1 .1 .75 0 0 0 0 0 .05], then this represents a 10% probability that the image is a 1, a 10% probability that the image is a 2, a 75% probability that the image is a 3, and a 5% probability that the image is a 9. \n","\n","\n","The way this fully connected layer works is that it looks at the output of the previous layer (which as we remember should represent the activation maps of high level features) and determines which features most correlate to a particular class. For example, if the program is predicting that some image is a dog, it will have high values in the activation maps that represent high level features like a paw or 4 legs, etc. Similarly, if the program is predicting that some image is a bird, it will have high values in the activation maps that represent high level features like wings or a beak, etc. Basically, a FC layer looks at what high level features most strongly correlate to a particular class and has particular weights so that when you compute the products between the weights and the previous layer, you get the correct probabilities for the different classes."]},{"cell_type":"markdown","metadata":{"id":"UD82OZgt2VFP","colab_type":"text"},"source":["**The learning process, Is it the same as our previous trainings?**\n","\n","Simply yes ... The way the computer is able to adjust its filter values (or weights) is through a backpropagation. backpropagation can be separated into 4 distinct sections, the forward pass, the loss function, the backward pass, and the weight update. During the forward pass, you take a training image which as we remember is a 32 x 32 x 3 array of numbers and pass it through the whole network. On our first training example, since all of the weights or filter values were randomly initialized, the output will probably be something like [.1 .1 .1 .1 .1 .1 .1 .1 .1 .1], basically an output that doesn’t give preference to any number in particular. The network, with its current weights, isn’t able to look for those low level features or thus isn’t able to make any reasonable conclusion about what the classification might be. This goes to the loss function part of backpropagation. Remember that what we are using right now is training data. This data has both an image and a label. Let’s say for example that the first training image inputted was a 3. The label for the image would be [0 0 0 1 0 0 0 0 0 0]. A loss function can be defined in many different ways but a common one is MSE (mean squared error), which is ½ times (actual - predicted) squared."]},{"cell_type":"markdown","metadata":{"id":"jOD71smI3AtP","colab_type":"text"},"source":["**Lets get our hands into the real stuff!**\n","\n","lets try to train our first conv neural network to classify images. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m7KBpffWzlxH"},"source":["# Example"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iAve6DCL4JH4","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf\n","\n","from tensorflow.keras import datasets, layers, models\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jRFxccghyMVo"},"source":["### Download and prepare the CIFAR10 dataset\n","\n","\n","The CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JWoEqyMuXFF4","colab":{}},"source":["(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n","plt.imshow(train_images[1])\n","print(train_labels[1])\n","# Normalize pixel values to be between 0 and 1\n","train_images, test_images = train_images / 255.0, test_images / 255.0\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7wArwCTJJlUa"},"source":["### Verify the data\n","\n","To verify that the dataset looks correct, let's plot the first 25 images from the training set and display the class name below each image.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"K3PAELE2eSU9","colab":{}},"source":["class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n","               'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(True)\n","    plt.imshow(train_images[i], cmap=plt.cm.binary)\n","    # The CIFAR labels happen to be arrays, \n","    # which is why you need the extra index\n","    plt.xlabel(class_names[train_labels[i][0]])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Oewp-wYg31t9"},"source":["**bold text**### Create the convolutional base"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3hQvqXpNyN3x"},"source":["The 6 lines of code below define the convolutional base using a common pattern: a stack of [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [MaxPooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers. The conv we use is 2D because it scans an image in the 2D domain, 3D conv can be used when considering time series and anything with dimension more than 2. The activation (or the output) of every Conv is sparse and not all its values are meaningful. therefore we have to filterout those unwanted numbers, MAX pooling does this job as it eleminates those numbers and keep the highest activations only.\n","\n","As input, a CNN takes tensors of shape (image_height, image_width, color_channels), ignoring the batch size. If you are new to these dimensions, color_channels refers to (R,G,B). In this example, you will configure our CNN to process inputs of shape (32, 32, 3), which is the format of CIFAR images. You can do this by passing the argument `input_shape` to our first layer.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"L9YmGQBQPrdn","colab":{}},"source":["model = models.Sequential()\n","model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))# default value of stride is 1\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))# p=0 s = 1\n","model.add(layers.MaxPooling2D((2, 2)))\n","model.add(layers.Conv2D(64, (3, 3), activation='relu'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lvDVFkg-2DPm"},"source":["Let's display the architecture of our model so far."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8-C4XBg4UTJy","colab":{}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_j-AXYeZ2GO5"},"source":["Above, you can see that the output of every Conv2D and MaxPooling2D layer is a 3D tensor of shape (height, width, channels). The width and height dimensions tend to shrink as you go deeper in the network. The number of output channels for each Conv2D layer is controlled by the first argument (e.g., 32 or 64). Typically,  as the width and height shrink, you can afford (computationally) to add more output channels in each Conv2D layer."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_v8sVOtG37bT"},"source":["### Add Dense layers on top\n","To complete our model, you will feed the last output tensor from the convolutional base (of shape (3, 3, 64)) into one or more Dense layers to perform classification. Dense layers take vectors as input (which are 1D), while the current output is a 3D tensor. First, you will flatten (or unroll) the 3D output to 1D,  then add one or more Dense layers on top. CIFAR has 10 output classes, so you use a final Dense layer with 10 outputs and a softmax activation."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mRs95d6LUVEi","colab":{}},"source":["model.add(layers.Flatten())\n","model.add(layers.Dense(512, activation='relu'))\n","model.add(layers.Dense(10, activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ipGiQMcR4Gtq"},"source":["Here's the complete architecture of our model."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8Yu_m-TZUWGX","colab":{}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xNKXi-Gy3RO-"},"source":["As you can see, our (3, 3, 64) outputs were flattened into vectors of shape (576) before going through two Dense layers.\n","\n","Later we will start the training process. in our training we use Adam optimizer to make sure our training, learning rate and hyper parameters are manged probably. there are other optimizers like SGD and GD but for this training we are going to use ADAM as it is fast and efficent for such task.\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P3odqfHP4M67"},"source":["### Compile and train the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MdDzI75PUXrG","colab":{}},"source":["model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","history = model.fit(train_images, train_labels, epochs=10, \n","                    validation_data=(test_images, test_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jKgyC5K_4O0d"},"source":["### Evaluate the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gtyDF0MKUcM7","colab":{}},"source":["plt.plot(history.history['accuracy'], label='accuracy')\n","plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.ylim([0.5, 1])\n","plt.legend(loc='lower right')\n","\n","test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0LvwaKhtUdOo","colab":{}},"source":["print(test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"8cfJ8AR03gT5"},"source":["Our simple CNN has achieved a test accuracy of over 70%. Not bad for a few lines of code! Away from numbers, lets try to evaluate the model inference abilities."]},{"cell_type":"code","metadata":{"id":"frcC9bPu5mGP","colab_type":"code","colab":{}},"source":["import random,numpy\n","iEvalIx= random.randrange(len(test_labels)-1)\n","iClassIX =test_labels[iEvalIx][0]\n","plt.imshow(test_images[iEvalIx], cmap=plt.cm.binary)\n","iPredictionIx = numpy.argmax(model.predict(test_images[iEvalIx].reshape(1,32,32,3)))\n","print (\"I think this is a\", class_names[iPredictionIx]) "],"execution_count":null,"outputs":[]}]}