{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Image Segmentation.ipynb","provenance":[{"file_id":"13bGFqUld16SCh9OuumHMpPk-xB5PnXHA","timestamp":1593237085940},{"file_id":"1llSzHAr6Cd_pUvg1WATXVZoTvICOo-zH","timestamp":1582640152325},{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/segmentation.ipynb","timestamp":1582639112488}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rCSP-dbMw88x"},"source":["# Image segmentation\n","\n","Image segmentation is a critical process in computer vision. It involves dividing a visual input into segments to simplify image analysis. Segments represent objects or parts of objects, and comprise sets of pixels, or “super-pixels”. Image segmentation sorts pixels into larger components, eliminating the need to consider individual pixels as units of observation. There are three levels of image analysis:\n","\n","1.   Classification – categorizing the entire image into a class such as “people”, “animals”, “outdoors”\n","\n","2.   Object detection – detecting objects within an image and drawing a rectangle around them, for example, a person or a sheep\n","\n","3.  Segmentation – identifying parts of the image and understanding what object they belong to. Segmentation lays the basis for performing object detection and classification. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gWMr3Ne5zMe1","colab_type":"text"},"source":["## Image segmentation types\n","Within the segmentation process itself, there are two levels of granularity:\n","\n","1. Semantic segmentation—classifies all the pixels of an image into meaningful classes of objects. These classes are “semantically interpretable” and correspond to real-world categories. For instance, you could isolate all the pixels associated with a cat and color them green. This is also known as dense prediction because it predicts the meaning of each pixel.\n","2.   Instance segmentation—identifies each instance of each object in an image. It differs from semantic segmentation in that it doesn’t categorize every pixel. If there are three cars in an image, semantic segmentation classifies all the cars as one instance, while instance segmentation identifies each individual car.\n","  \n","  ![Semantic vs Instance segmentation](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2019/03/Screenshot-from-2019-03-28-12-08-09.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Yy5EfpFBzj0A","colab_type":"text"},"source":["## Image segmentation- now and then\n","Previously, segmentation problems were tackled by conventional methods such as Thresholding, K-means clustering, Histogram based image segmentation and Edge detection. In this Colab we will practice solving a segmentation problem via deep learning. The state of the art architectures are novel and specialized for variety of purposes. They use Convolutional Neural Networks (CNNs), Fully Convolutional Networks (FCNs), Ensemble learning , DeepLab and SegNet neural network."]},{"cell_type":"markdown","metadata":{"id":"nek_KoNA1QPm","colab_type":"text"},"source":["## Image segmentation - Applications\n","These applications involve identifying object instances of a specific class in a digital image. Semantic objects can be classified into classes like human faces, cars, buildings, or cats. Applications that capture and process images to provide operational guidance to devices. This includes both industrial and non-industrial applications. Machine vision systems use digital sensors in specialized cameras that allow computer hardware and software to measure, process, and analyze images. For example, an inspection system photographs soda bottles and then analyzes the images according to pass-fail criteria to determine if the bottles are properly filled. Also it serves well for medical purposes and face authentication.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oBxt2NfE1pnV","colab_type":"text"},"source":["Let’s get into our first segmentation problem, it is animal sematic segmentation, in this model, unlike other sessions, we won’t be building our network from scratch as it is hard to create a working network and keep it illustratable using a colab. yet you can always use your own design, just keep in mind you may need more computation power to reach the optimum weights of the exported models. We will keep it all clear!\n","\n","Lets start by downloading our example package.\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MQmKthrSBCld","colab":{}},"source":["!pip install git+https://github.com/tensorflow/examples.git\n","\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"g87--n2AtyO_","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","from tensorflow_examples.models.pix2pix import pix2pix\n","\n","import tensorflow_datasets as tfds\n","tfds.disable_progress_bar()\n","import numpy\n","from IPython.display import clear_output\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oWe0_rQM4JbC"},"source":["## Download the Oxford-IIIT Pets dataset\n","\n","The dataset is already included in TensorFlow datasets, all that is needed to do is download it. The segmentation masks are included in version 3+. Details of dataset can be found [here](https://www.robots.ox.ac.uk/~vgg/data/pets/)."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"40ITeStwDwZb","colab":{}},"source":["!python -m tensorflow_datasets.scripts.download_and_prepare --register_checksums --datasets=oxford_iiit_pet:3.1.0\n","dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n","\n","# !pip install git+https://github.com/tensorflow/datasets.git\n","# import tensorflow_datasets as tfds\n","# # dataset, info = tfds.load('oxford_iiit_pet:3.0.0', with_info=True)\n","# dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XpFGV0-qb3oO","colab_type":"text"},"source":["# New Section"]},{"cell_type":"markdown","metadata":{"id":"K3hfaCFvb4DW","colab_type":"text"},"source":["# New Section"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rJcVdj_U4vzf"},"source":["The following code performs a simple augmentation of flipping an image. In addition,  image is normalized to [0,1]. In the Oxford-IIIT Pets dataset, the segmentation masks are labeled either {1, 2, 3}. For the sake of convenience, let's subtract 1 from the segmentation mask, resulting in labels that are : {0, 1, 2}."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FD60EbcAQqov","colab":{}},"source":["def normalize(input_image, input_mask):\n","  input_image = tf.cast(input_image, tf.float32) / 255.0\n","  input_mask -= 1\n","  return input_image, input_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2NPlCnBXQwb1","colab":{}},"source":["# @tf.function\n","import tensorflow as tf\n","def load_image_train(datapoint):\n","  input_image = tf.image.resize(datapoint['image'], (128, 128))\n","  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n","\n","  if tf.random.uniform(()) > 0.5:\n","    input_image = tf.image.flip_left_right(input_image)\n","    input_mask = tf.image.flip_left_right(input_mask)\n","\n","  input_image, input_mask = normalize(input_image, input_mask)\n","\n","  return input_image, input_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zf0S67hJRp3D","colab":{}},"source":["def load_image_test(datapoint):\n","  input_image = tf.image.resize(datapoint['image'], (128, 128))\n","  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n","\n","  input_image, input_mask = normalize(input_image, input_mask)\n","\n","  return input_image, input_mask"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"65-qHTjX5VZh"},"source":["The dataset already contains the required splits of test and train and so let's continue to use the same split."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yHwj2-8SaQli","colab":{}},"source":["TRAIN_LENGTH = info.splits['train'].num_examples\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 1000\n","STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"39fYScNz9lmo","colab":{}},"source":["train = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","test = dataset['test'].map(load_image_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DeFwFDN6EVoI","colab":{}},"source":["train_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n","train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","test_dataset = test.batch(BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Xa3gMAE_9qNa"},"source":["Let's take a look at an image example and it's correponding mask from the dataset."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3N2RPAAW9q4W","colab":{}},"source":["def Projection(image1, image2, transparency):\n","  image1 = numpy.asarray(image1)\n","  image2 = numpy.asarray(image2)\n","  ZIka = image1.copy()\n","  h, w, c = image1.shape\n","  for y in range(h):\n","    for x in range(w):\n","        for z in range(c):\n","          ZIka[y][x][z] = image1[y][x][z] * transparency \\\n","          + (image2[y][x] * (1 - transparency))\n","  return ZIka\n","   \n","def display(display_list):\n","  plt.figure(figsize=(15, 15))\n","\n","  title = ['Input Image', 'True Mask', 'Predicted Mask']\n","\n","  for i in range(len(display_list)):\n","    plt.subplot(1, len(display_list), i+1)\n","    plt.title(title[i])\n","    if i > 0:\n","      img = Projection(tf.keras.preprocessing.image.array_to_img(display_list[0]),\n","                       tf.keras.preprocessing.image.array_to_img(display_list[i]),\n","                       0.2)\n","    else:\n","      img = tf.keras.preprocessing.image.array_to_img(display_list[i])\n","    plt.imshow(img)\n","    plt.axis('off')\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"a6u_Rblkteqb","colab":{}},"source":["for image, mask in train.take(500):\n","  sample_image, sample_mask = image, mask\n","display([sample_image, sample_mask])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FAOe93FRMk3w"},"source":["## Define the model\n","The model being used here is a modified U-Net. A U-Net consists of an encoder (downsampler) and decoder (upsampler). In-order to learn robust features, and reduce the number of trainable parameters, a pretrained model can be used as the encoder. Thus, the encoder for this task will be a pretrained MobileNetV2 model, whose intermediate outputs will be used, and the decoder will be the upsample block already implemented in TensorFlow Examples in the [Pix2pix tutorial](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). \n","\n","The reason to output three channels is because there are three possible labels for each pixel. Think of this as multi-classification where each pixel is being classified into three classes."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"c6iB4iMvMkX9","colab":{}},"source":["OUTPUT_CHANNELS = 3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"W4mQle3lthit"},"source":["As mentioned, the encoder will be a pretrained MobileNetV2 model which is prepared and ready to use in [tf.keras.applications](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications). The encoder consists of specific outputs from intermediate layers in the model. Note that the encoder will not be trained during the training process."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"liCeLH0ctjq7","colab":{}},"source":["base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n","\n","# Use the activations of these layers\n","layer_names = [\n","    'block_1_expand_relu',   # 64x64\n","    'block_3_expand_relu',   # 32x32\n","    'block_6_expand_relu',   # 16x16\n","    'block_13_expand_relu',  # 8x8\n","    'block_16_project',      # 4x4\n","]\n","layers = [base_model.get_layer(name).output for name in layer_names]\n","\n","# Create the feature extraction model\n","down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n","\n","down_stack.trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KPw8Lzra5_T9"},"source":["The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p0ZbfywEbZpJ","colab":{}},"source":["up_stack = [\n","    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n","    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n","    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n","    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"45HByxpVtrPF","colab":{}},"source":["def unet_model(output_channels):\n","  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n","  x = inputs\n","\n","  # Downsampling through the model\n","  skips = down_stack(x)\n","  x = skips[-1]\n","  skips = reversed(skips[:-1])\n","\n","  # Upsampling and establishing the skip connections\n","  for up, skip in zip(up_stack, skips):\n","    x = up(x)\n","    concat = tf.keras.layers.Concatenate()\n","    x = concat([x, skip])\n","\n","  # This is the last layer of the model\n","  last = tf.keras.layers.Conv2DTranspose(\n","      output_channels, 3, strides=2,\n","      padding='same')  #64x64 -> 128x128\n","\n","  x = last(x)\n","\n","  return tf.keras.Model(inputs=inputs, outputs=x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j0DGH_4T0VYn"},"source":["## Train the model\n","Now, all that is left to do is to compile and train the model. The loss being used here is `losses.SparseCategoricalCrossentropy(from_logits=True)`. The reason to use this loss function is because the network is trying to assign each pixel a label, just like multi-class prediction. In the true segmentation mask, each pixel has either a {0,1,2}. The network here is outputting three channels. Essentially, each channel is trying to learn to predict a class, and `losses.SparseCategoricalCrossentropy(from_logits=True)` is the recommended loss for \n","such a scenario. Using the output of the network, the label assigned to the pixel is the channel with the highest value. This is what the create_mask function is doing."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6he36HK5uKAc","colab":{}},"source":["model = unet_model(OUTPUT_CHANNELS)\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xVMzbIZLcyEF"},"source":["Have a quick look at the resulting model architecture:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sw82qF1Gcovr","colab":{}},"source":["tf.keras.utils.plot_model(model, show_shapes=True)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Tc3MiEO2twLS"},"source":["Let's try out the model to see what it predicts before training."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UwvIKLZPtxV_","colab":{}},"source":["def create_mask(pred_mask):\n","  pred_mask = tf.argmax(pred_mask, axis=-1)\n","  pred_mask = pred_mask[..., tf.newaxis]\n","  return pred_mask[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YLNsrynNtx4d","colab":{}},"source":["def show_predictions(dataset=None, num=1):\n","  if dataset:\n","    for image, mask in dataset.take(num):\n","      pred_mask = model.predict(image)\n","      display([image[0], mask[0], create_mask(pred_mask)])\n","  else:\n","    display([sample_image, sample_mask,\n","             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"X_1CC0T4dho3","colab":{}},"source":["show_predictions()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"22AyVYWQdkgk"},"source":["Let's observe how the model improves while it is training. To accomplish this task, a callback function is defined below. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wHrHsqijdmL6","colab":{}},"source":["class DisplayCallback(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs=None):\n","    clear_output(wait=True)\n","    show_predictions()\n","    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"StKDH_B9t4SD","colab":{}},"source":["EPOCHS = 20\n","VAL_SUBSPLITS = 5\n","VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n","\n","model_history = model.fit(train_dataset, epochs=EPOCHS,\n","                          steps_per_epoch=STEPS_PER_EPOCH,\n","                          validation_steps=VALIDATION_STEPS,\n","                          validation_data=test_dataset,\n","                          callbacks=[DisplayCallback()])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"P_mu0SAbt40Q","colab":{}},"source":["loss = model_history.history['loss']\n","val_loss = model_history.history['val_loss']\n","\n","accuracy = model_history.history['accuracy']\n","val_accuracy = model_history.history['val_accuracy']\n","\n","epochs = range(EPOCHS)\n","\n","plt.figure()\n","plt.plot(epochs, accuracy, 'r', label='Training accuracy')\n","plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n","plt.title('Training and Validation accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy Value')\n","plt.ylim([0, 1])\n","plt.legend()\n","plt.show()\n","\n","plt.figure()\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","plt.plot(epochs, val_loss, 'g', label='Validation loss')\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss Value')\n","plt.ylim([0, 1])\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"unP3cnxo_N72"},"source":["## Make predictions"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7BVXldSo-0mW"},"source":["Let's make some predictions. In the interest of saving time, the number of epochs was kept small, but you may set this higher to achieve more accurate results."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ikrzoG24qwf5","colab":{}},"source":["show_predictions(test_dataset, 3)"],"execution_count":null,"outputs":[]}]}